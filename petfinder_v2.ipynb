{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Bring in base datasets and append columns from sentiment JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load relevant packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "#define path where files, folders, and subfolders exist\n",
    "initPath = \"/Users/jaredoeth/Document/petfinder-adoption-prediction\"\n",
    "os.chdir(initPath)\n",
    "\n",
    "#loading base data sets\n",
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')\n",
    "\n",
    "#function for bringing in sentiment data\n",
    "def dfAddSentiment(split):\n",
    "    newPath = initPath + '/'+split+'_sentiment/'\n",
    "    tempList = []\n",
    "\n",
    "    #cycle through JSON files and glean document sentiment and language\n",
    "    for filename in os.listdir(newPath):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(newPath, filename)) as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            petID = os.path.splitext(filename)[0]\n",
    "            row = [petID, data['documentSentiment']['magnitude'], data['documentSentiment']['score'], data['language'], len(data['sentences']), len(data['entities'])]\n",
    "            tempList.append(row)\n",
    "    \n",
    "    labels = ['PetID', 'Desc_Magnitude', 'Desc_Score', 'Desc_Language', 'Desc_numSentences', 'Desc_numEntities']\n",
    "    df = pd.DataFrame(tempList, columns=labels)\n",
    "    \n",
    "    #left join sentiment to base dataframes\n",
    "    if split == 'train':\n",
    "        dfBase = dfTrain\n",
    "    elif split == 'test':\n",
    "        dfBase = dfTest\n",
    "    else:\n",
    "        print('invalid function parameter')\n",
    "        \n",
    "    dfOut = pd.merge(dfBase, df, on='PetID', how='left')\n",
    "    \n",
    "    #move target variable to end in train dataset\n",
    "    if split == 'train':\n",
    "        colList = dfOut.columns.tolist()\n",
    "        colList.remove('AdoptionSpeed')\n",
    "        colList.append('AdoptionSpeed')\n",
    "        dfOut = dfOut[colList]\n",
    "\n",
    "    #drop description text column\n",
    "    dfOut = dfOut.drop(['Description'], axis=1)\n",
    "    \n",
    "    return dfOut\n",
    "\n",
    "dfTrain = dfAddSentiment('train')\n",
    "dfTest = dfAddSentiment('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Basic Image Statistics using method from https://www.kaggle.com/kaerunantoka/extract-image-features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image as IMG\n",
    "\n",
    "def getSize(filename):\n",
    "    #filename = images_path + filename\n",
    "    st = os.stat(filename)\n",
    "    return st.st_size\n",
    "\n",
    "def getDimensions(filename):\n",
    "    #filename = images_path + filename\n",
    "    img_size = IMG.open(filename).size\n",
    "    return img_size \n",
    "\n",
    "def getImageStats(split):\n",
    "    imagePath = initPath + '/'+split+'_images/'\n",
    "    image_files = sorted(glob.glob(imagePath+'*.jpg'))\n",
    "    \n",
    "    df_imgs = pd.DataFrame(image_files, columns=['image_filename'])\n",
    "    imgs_pets = df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0]) #PetID\n",
    "\n",
    "    df_imgs = df_imgs.assign(PetID=imgs_pets)\n",
    "\n",
    "    df_imgs['image_size'] = df_imgs['image_filename'].apply(getSize)\n",
    "    df_imgs['temp_size'] = df_imgs['image_filename'].apply(getDimensions)\n",
    "    df_imgs['width'] = df_imgs['temp_size'].apply(lambda x : x[0])\n",
    "    df_imgs['height'] = df_imgs['temp_size'].apply(lambda x : x[1])\n",
    "    df_imgs = df_imgs.drop(['temp_size'], axis=1)\n",
    "\n",
    "    #aggregate to one row per pet\n",
    "    aggs = {\n",
    "        'image_size': ['min', 'max', 'mean', 'median', \"sum\"],\n",
    "        'width': ['min', 'max', 'mean', 'median', \"sum\"],\n",
    "        'height': ['min', 'max', 'mean', 'median', \"sum\"],\n",
    "    }\n",
    "\n",
    "    agg_imgs = df_imgs.groupby('PetID').agg(aggs)\n",
    "\n",
    "    new_columns = [\n",
    "        k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
    "    ]\n",
    "\n",
    "    agg_imgs.columns = new_columns\n",
    "\n",
    "    agg_imgs = agg_imgs.reset_index()\n",
    "    \n",
    "    #left join sentiment to base dataframes\n",
    "    if split == 'train':\n",
    "        dfBase = dfTrain\n",
    "    elif split == 'test':\n",
    "        dfBase = dfTest\n",
    "    else:\n",
    "        print('invalid function parameter')\n",
    "        \n",
    "    dfOut = pd.merge(dfBase, agg_imgs, on='PetID', how='left')\n",
    "    \n",
    "    #move target variable to end in train dataset\n",
    "    if split == 'train':\n",
    "        colList = dfOut.columns.tolist()\n",
    "        colList.remove('AdoptionSpeed')\n",
    "        colList.append('AdoptionSpeed')\n",
    "        dfOut = dfOut[colList]\n",
    "        \n",
    "    return(dfOut)\n",
    "\n",
    "dfTrain = getImageStats('train')\n",
    "dfTest = getImageStats('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. More complex image features using method from https://www.kaggle.com/teemingyi/image-statistics-for-petfinder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import itemfreq\n",
    "from scipy import ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import feature\n",
    "from PIL import Image as IMG\n",
    "import operator\n",
    "import cv2\n",
    "\n",
    "\n",
    "def color_analysis(img):\n",
    "    # obtain the color palette of the image \n",
    "    palette = defaultdict(int)\n",
    "    for pixel in img.getdata():\n",
    "        palette[pixel] += 1\n",
    "    \n",
    "    # sort the colors present in the image \n",
    "    sorted_x = sorted(palette.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    \n",
    "    light_shade, dark_shade, shade_count, pixel_limit = 0, 0, 0, 1000\n",
    "    for i, x in enumerate(sorted_x[:pixel_limit]):\n",
    "        if all(xx <= 20 for xx in x[0][:3]): ## dull : too much darkness \n",
    "            dark_shade += x[1]\n",
    "        if all(xx >= 240 for xx in x[0][:3]): ## bright : too much whiteness \n",
    "            light_shade += x[1]\n",
    "        shade_count += x[1]\n",
    "        \n",
    "    light_percent = round((float(light_shade)/shade_count)*100, 2)\n",
    "    dark_percent = round((float(dark_shade)/shade_count)*100, 2)\n",
    "    return light_percent, dark_percent\n",
    "\n",
    "def perform_color_analysis(img):\n",
    "\n",
    "    path = imagePath + img \n",
    "    im = IMG.open(path) #.convert(\"RGB\")\n",
    "    \n",
    "    # cut the images into two halves as complete average may give bias results\n",
    "    size = im.size\n",
    "    halves = (size[0]/2, size[1]/2)\n",
    "    im1 = im.crop((0, 0, size[0], halves[1]))\n",
    "    im2 = im.crop((0, halves[1], size[0], size[1]))\n",
    "\n",
    "    try:\n",
    "        light_percent1, dark_percent1 = color_analysis(im1)\n",
    "        light_percent2, dark_percent2 = color_analysis(im2)\n",
    "    except Exception as e:\n",
    "        light_percent1, dark_percent1 = -1, -1\n",
    "        light_percent2, dark_percent2 = -1, -1\n",
    "\n",
    "    light_percent = (light_percent1 + light_percent2)/2 \n",
    "    dark_percent = (dark_percent1 + dark_percent2)/2 \n",
    "    \n",
    "    return dark_percent, light_percent\n",
    "\n",
    "def average_pixel_width(img):\n",
    "    path = imagePath + img \n",
    "    im = IMG.open(path)    \n",
    "    im_array = np.asarray(im.convert(mode='L'))\n",
    "    edges_sigma1 = feature.canny(im_array, sigma=3)\n",
    "    apw = (float(np.sum(edges_sigma1)) / (im.size[0]*im.size[1]))\n",
    "    return apw*100\n",
    "\n",
    "def get_blurrness_score(image):\n",
    "    path =  imagePath + image \n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "    return fm\n",
    "\n",
    "def getImageDetail(split):\n",
    "    \n",
    "    imagePath = initPath + '/'+split+'_images/'\n",
    "    \n",
    "    imgs = os.listdir(imagePath)\n",
    "    features = pd.DataFrame(data=imgs, columns=['image'])\n",
    "\n",
    "    #keep only display image or the one ending in \"-1\"\n",
    "    features = features.loc[['-1.' in x for x in features.image]]\n",
    "\n",
    "    features['dullness_whiteness'] = features['image'].apply(lambda x : perform_color_analysis(x))\n",
    "    features['dullness'] = features.dullness_whiteness.map(lambda x: x[0])\n",
    "    features['whiteness'] = features.dullness_whiteness.map(lambda x: x[1])\n",
    "    features['average_pixel_width'] = features['image'].apply(average_pixel_width)\n",
    "    features['blurrness'] = features['image'].apply(get_blurrness_score)\n",
    "\n",
    "    #create PetID from image name and drop image name and dullness_whiteness\n",
    "    imgs_pets2 = features['image'].apply(lambda x: x.split('/')[-1].split('-')[0]) #PetID\n",
    "    features.insert(0,'PetID',imgs_pets2)\n",
    "    features = features.drop(['image','dullness_whiteness'], axis=1)\n",
    "    \n",
    "    #left join sentiment to base dataframes\n",
    "    if split == 'train':\n",
    "        dfBase = dfTrain\n",
    "    elif split == 'test':\n",
    "        dfBase = dfTest\n",
    "    else:\n",
    "        print('invalid function parameter')\n",
    "        \n",
    "    dfOut = pd.merge(dfBase, features, on='PetID', how='left')\n",
    "    \n",
    "    #move target variable to end in train dataset\n",
    "    if split == 'train':\n",
    "        colList = dfOut.columns.tolist()\n",
    "        colList.remove('AdoptionSpeed')\n",
    "        colList.append('AdoptionSpeed')\n",
    "        dfOut = dfOut[colList]\n",
    "        \n",
    "    return(dfOut)\n",
    "\n",
    "imagePath = initPath + '/train_images/'\n",
    "dfTrain = getImageDetail('train')\n",
    "\n",
    "imagePath = initPath + '/test_images/'\n",
    "dfTest = getImageDetail('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional. Output to csv for review and copy dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.to_csv('dfTrain.csv', sep='\\t', encoding='utf-8', index=False, header=True)\n",
    "dfTest.to_csv('dfTest.csv', sep='\\t', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "#copy data frames in case cleaning goes wrong\n",
    "dfTrainBackup = dfTrain \n",
    "dfTestBackup = dfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset data frames to backup if cleaning goes wrong\n",
    "dfTrain = dfTrainBackup\n",
    "dfTest = dfTestBackup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4a. Check where nulls exist for clean-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type                      0\n",
      "Name                   1257\n",
      "Age                       0\n",
      "Breed1                    0\n",
      "Breed2                    0\n",
      "Gender                    0\n",
      "Color1                    0\n",
      "Color2                    0\n",
      "Color3                    0\n",
      "MaturitySize              0\n",
      "FurLength                 0\n",
      "Vaccinated                0\n",
      "Dewormed                  0\n",
      "Sterilized                0\n",
      "Health                    0\n",
      "Quantity                  0\n",
      "Fee                       0\n",
      "State                     0\n",
      "RescuerID                 0\n",
      "VideoAmt                  0\n",
      "PetID                     0\n",
      "PhotoAmt                  0\n",
      "Desc_Magnitude          551\n",
      "Desc_Score              551\n",
      "Desc_Language           551\n",
      "Desc_numSentences       551\n",
      "Desc_numEntities        551\n",
      "image_size_min          341\n",
      "image_size_max          341\n",
      "image_size_mean         341\n",
      "image_size_median       341\n",
      "image_size_sum          341\n",
      "width_min               341\n",
      "width_max               341\n",
      "width_mean              341\n",
      "width_median            341\n",
      "width_sum               341\n",
      "height_min              341\n",
      "height_max              341\n",
      "height_mean             341\n",
      "height_median           341\n",
      "height_sum              341\n",
      "dullness                341\n",
      "whiteness               341\n",
      "average_pixel_width     341\n",
      "blurrness               341\n",
      "AdoptionSpeed             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dfTrain.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b. Check column cardinality for categorical columns (before we do one hot encoding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           column  cardinality\n",
      "0            Type            2\n",
      "1            Name         9061\n",
      "2          Breed1          176\n",
      "3          Breed2          135\n",
      "4          Gender            3\n",
      "5          Color1            7\n",
      "6          Color2            7\n",
      "7          Color3            6\n",
      "8    MaturitySize            4\n",
      "9       FurLength            3\n",
      "10     Vaccinated            3\n",
      "11       Dewormed            3\n",
      "12     Sterilized            3\n",
      "13         Health            3\n",
      "14          State           14\n",
      "15      RescuerID         5595\n",
      "16  Desc_Language            5\n"
     ]
    }
   ],
   "source": [
    "uniqueType = len(dfTrain.Type.unique())\n",
    "uniqueName = len(dfTrain.Name.unique())\n",
    "uniqueBreed1 = len(dfTrain.Breed1.unique())\n",
    "uniqueBreed2 = len(dfTrain.Breed2.unique())\n",
    "uniqueGender = len(dfTrain.Gender.unique())\n",
    "uniqueColor1 = len(dfTrain.Color1.unique())\n",
    "uniqueColor2 = len(dfTrain.Color2.unique())\n",
    "uniqueColor3 = len(dfTrain.Color3.unique())\n",
    "uniqueMaturitySize = len(dfTrain.MaturitySize.unique())\n",
    "uniqueFurLength = len(dfTrain.FurLength.unique())\n",
    "uniqueVaccinated = len(dfTrain.Vaccinated.unique())\n",
    "uniqueDewormed = len(dfTrain.Dewormed.unique())\n",
    "uniqueSterilized = len(dfTrain.Sterilized.unique())\n",
    "uniqueHealth = len(dfTrain.Health.unique())\n",
    "uniqueState = len(dfTrain.State.unique())\n",
    "uniqueRescuerID = len(dfTrain.RescuerID.unique())\n",
    "uniqueDesc_Language = len(dfTrain.Desc_Language.unique())\n",
    "\n",
    "colList = ['Type', 'Name', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'State', 'RescuerID', 'Desc_Language']\n",
    "data = []\n",
    "for col in colList:\n",
    "    val = eval('unique'+col)\n",
    "    data.append([col,val])\n",
    "    \n",
    "dfUniques = pd.DataFrame(data, columns=['column','cardinality'])\n",
    "\n",
    "print(dfUniques)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Preprocess data - impute values, encode categorical variables (label, binary, and one hot)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfTrain shape: (14993, 88)\n",
      "dfTest shape: (3948, 88)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "      \n",
    "def imputeMean(df,col):\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def imputeMode(df,col):\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    return df\n",
    "\n",
    "def imputeBlank(df,col):\n",
    "    df[col] = df[col].fillna('')\n",
    "    return df    \n",
    "\n",
    "def imputeNegOne(df,col):\n",
    "    df[col] = df[col].fillna(-1)\n",
    "    return df\n",
    "\n",
    "def labelEncode(df,col):\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "def oneHotEncodeMult(df,cols):   \n",
    "    #create one column for each unique member of col, removing the first one\n",
    "    dfOut = pd.get_dummies(df, prefix=cols, columns = cols, drop_first=True)\n",
    "    return dfOut\n",
    "\n",
    "def binaryEncodeMult(df,cols):\n",
    "    ce_bin = ce.BinaryEncoder(cols=cols)\n",
    "    dfOut = ce_bin.fit_transform(df)\n",
    "    return dfOut\n",
    "\n",
    "def hasName(df):\n",
    "    df['Name']=df['Name'].str.lower()\n",
    "    \n",
    "    #replace nulls with \"unknown\"\n",
    "    df['Name'].fillna('unknown', inplace = True) \n",
    "\n",
    "    #when any of the following criteria is met show 0 else 1. goal is to represent pets that have actual name with 1s\n",
    "    df['Name'] = ~df['Name'].str.contains('not named|no name|unknown|dog|cat|pup|kitt|1|2|3|4|5|6|7|8|9', regex=True)*1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    #drop pet id, rescuer id, and photo dimension sums columns\n",
    "    df = df.drop(['PetID','width_sum','height_sum','image_size_sum','RescuerID'], axis=1)\n",
    "\n",
    "    #impute nulls with mean\n",
    "    numCols = ['Health','Sterilized','Dewormed','Vaccinated','FurLength','MaturitySize']\n",
    "    \n",
    "    for col in numCols:\n",
    "        df = imputeMean(df,col)\n",
    "    \n",
    "    #impute nulls to -1 with goal to distinguish that these records do not have data\n",
    "    sentimentCols = ['Desc_Magnitude','Desc_Score','Desc_numSentences','Desc_numEntities']\n",
    "    photoCols = ['image_size_min','image_size_max','image_size_mean','image_size_median','width_min','width_max','width_mean','width_median','height_min','height_max','height_mean','height_median','dullness','whiteness','average_pixel_width','blurrness']\n",
    "    \n",
    "    for col in sentimentCols:\n",
    "        df = imputeNegOne(df,col)\n",
    "        \n",
    "    for col in photoCols:\n",
    "        df = imputeNegOne(df,col)\n",
    "        \n",
    "    #impute nulls to blank category, will be label encoded shortly\n",
    "    sentimentColsCat = ['Desc_Language']\n",
    "\n",
    "    for col in sentimentColsCat:\n",
    "        df = imputeBlank(df,col)\n",
    "    \n",
    "    #clean name column to 1's and 0's showing which records have pet name\n",
    "    df = hasName(df)\n",
    "\n",
    "    labelEncodeCols = ['Desc_Language']\n",
    "    # repeat this for all columns that require label encoding\n",
    "    for col in labelEncodeCols:\n",
    "        df = labelEncode(df,col)\n",
    "\n",
    "    oneHotEncodeCols = ['Desc_Language','State','Color3','Color2','Color1','Gender','Type']\n",
    "    df = oneHotEncodeMult(df,oneHotEncodeCols)\n",
    "    \n",
    "    #these columns have high cardinality, ranging from 135-9061 in our train dataset and would be detrimental to one hot encode because of the curse of dimensionality\n",
    "    \n",
    "    highCardCols = ['Breed2','Breed1']\n",
    "    # use binary encoding for high cardinality dimensions so we do not end up with 1000s of dimension columns\n",
    "    # curse of dimensionality\n",
    "    df = binaryEncodeMult(df,highCardCols)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "#reset dataframes to \"backup\" before running each time. this can be removed once cell is finalized\n",
    "dfTrain = dfTrainBackup\n",
    "dfTest = dfTestBackup\n",
    "\n",
    "#combine into one dataset for pre-processing with indicator column\n",
    "dfTrain['train']=1\n",
    "dfTest['train']=0\n",
    "dfTest['AdoptionSpeed'] = np.nan\n",
    "dfCombined = pd.concat([dfTrain, dfTest], sort=True)\n",
    "\n",
    "#preprocess dataset together so that columns are aligned\n",
    "dfCombined = preprocess(dfCombined)\n",
    "\n",
    "#separate back into two dataframes\n",
    "dfTrain = dfCombined[dfCombined.train == 1]\n",
    "dfTest = dfCombined[dfCombined.train == 0]\n",
    "\n",
    "#drop indicator column\n",
    "dfTrain = dfTrain.drop('train', axis = 1)\n",
    "dfTest = dfTest.drop('train', axis = 1)\n",
    "\n",
    "#show resulting shape\n",
    "print(\"dfTrain shape: \" + str(dfTrain.shape))\n",
    "print(\"dfTest shape: \" + str(dfTest.shape))\n",
    "\n",
    "dfTrain.to_csv('dfTrainPP.csv', sep='\\t', encoding='utf-8', index=False, header=True)\n",
    "dfTest.to_csv('dfTestPP.csv', sep='\\t', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Begin building models using dfTrain dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load relevant packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#define path where files, folders, and subfolders exist\n",
    "initPath = \"/Users/jaredoeth/Document/petfinder-adoption-prediction\"\n",
    "os.chdir(initPath)\n",
    "\n",
    "#pull files back from CSV so all of the building/preprocessing doesnt need to be re-run\n",
    "dfTrain = pd.read_csv('dfTrainPP.csv', sep='\\t', encoding='utf-8')\n",
    "dfTest = pd.read_csv('dfTestPP.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11994, 87) (11994, 1) (2999, 87) (2999, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025835</td>\n",
       "      <td>-0.111008</td>\n",
       "      <td>-0.185262</td>\n",
       "      <td>-0.206163</td>\n",
       "      <td>-0.329709</td>\n",
       "      <td>2.151359</td>\n",
       "      <td>2.791640</td>\n",
       "      <td>0.509129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.547485</td>\n",
       "      <td>-0.577029</td>\n",
       "      <td>-0.261376</td>\n",
       "      <td>-0.212885</td>\n",
       "      <td>3.980641</td>\n",
       "      <td>-0.216497</td>\n",
       "      <td>-0.216286</td>\n",
       "      <td>-0.975618</td>\n",
       "      <td>-0.412765</td>\n",
       "      <td>1.091552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025835</td>\n",
       "      <td>-0.111008</td>\n",
       "      <td>5.397748</td>\n",
       "      <td>-0.206163</td>\n",
       "      <td>-0.329709</td>\n",
       "      <td>-0.464823</td>\n",
       "      <td>2.791640</td>\n",
       "      <td>-1.964138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.547485</td>\n",
       "      <td>-0.577029</td>\n",
       "      <td>3.825905</td>\n",
       "      <td>-0.212885</td>\n",
       "      <td>-0.251216</td>\n",
       "      <td>-0.216497</td>\n",
       "      <td>-0.216286</td>\n",
       "      <td>-0.975618</td>\n",
       "      <td>-0.412765</td>\n",
       "      <td>-0.916127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025835</td>\n",
       "      <td>-0.111008</td>\n",
       "      <td>-0.185262</td>\n",
       "      <td>-0.206163</td>\n",
       "      <td>-0.329709</td>\n",
       "      <td>-0.464823</td>\n",
       "      <td>-0.358212</td>\n",
       "      <td>0.509129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.547485</td>\n",
       "      <td>-0.577029</td>\n",
       "      <td>-0.261376</td>\n",
       "      <td>-0.212885</td>\n",
       "      <td>-0.251216</td>\n",
       "      <td>-0.216497</td>\n",
       "      <td>-0.216286</td>\n",
       "      <td>-0.975618</td>\n",
       "      <td>-0.412765</td>\n",
       "      <td>-0.916127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025835</td>\n",
       "      <td>-0.111008</td>\n",
       "      <td>-0.185262</td>\n",
       "      <td>-0.206163</td>\n",
       "      <td>-0.329709</td>\n",
       "      <td>-0.464823</td>\n",
       "      <td>-0.358212</td>\n",
       "      <td>0.509129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.826534</td>\n",
       "      <td>-0.577029</td>\n",
       "      <td>3.825905</td>\n",
       "      <td>-0.212885</td>\n",
       "      <td>-0.251216</td>\n",
       "      <td>-0.216497</td>\n",
       "      <td>-0.216286</td>\n",
       "      <td>1.024991</td>\n",
       "      <td>-0.412765</td>\n",
       "      <td>1.091552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.025835</td>\n",
       "      <td>-0.111008</td>\n",
       "      <td>-0.185262</td>\n",
       "      <td>-0.206163</td>\n",
       "      <td>-0.329709</td>\n",
       "      <td>-0.464823</td>\n",
       "      <td>-0.358212</td>\n",
       "      <td>0.509129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.826534</td>\n",
       "      <td>1.733014</td>\n",
       "      <td>-0.261376</td>\n",
       "      <td>-0.212885</td>\n",
       "      <td>-0.251216</td>\n",
       "      <td>-0.216497</td>\n",
       "      <td>-0.216286</td>\n",
       "      <td>-0.975618</td>\n",
       "      <td>-0.412765</td>\n",
       "      <td>-0.916127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6         7   \\\n",
       "0  0.0 -0.025835 -0.111008 -0.185262 -0.206163 -0.329709  2.151359  2.791640   \n",
       "1  0.0 -0.025835 -0.111008  5.397748 -0.206163 -0.329709 -0.464823  2.791640   \n",
       "2  0.0 -0.025835 -0.111008 -0.185262 -0.206163 -0.329709 -0.464823 -0.358212   \n",
       "3  0.0 -0.025835 -0.111008 -0.185262 -0.206163 -0.329709 -0.464823 -0.358212   \n",
       "4  0.0 -0.025835 -0.111008 -0.185262 -0.206163 -0.329709 -0.464823 -0.358212   \n",
       "\n",
       "         8    9     ...           77        78        79        80        81  \\\n",
       "0  0.509129  0.0    ...    -0.547485 -0.577029 -0.261376 -0.212885  3.980641   \n",
       "1 -1.964138  0.0    ...    -0.547485 -0.577029  3.825905 -0.212885 -0.251216   \n",
       "2  0.509129  0.0    ...    -0.547485 -0.577029 -0.261376 -0.212885 -0.251216   \n",
       "3  0.509129  0.0    ...     1.826534 -0.577029  3.825905 -0.212885 -0.251216   \n",
       "4  0.509129  0.0    ...     1.826534  1.733014 -0.261376 -0.212885 -0.251216   \n",
       "\n",
       "         82        83        84        85        86  \n",
       "0 -0.216497 -0.216286 -0.975618 -0.412765  1.091552  \n",
       "1 -0.216497 -0.216286 -0.975618 -0.412765 -0.916127  \n",
       "2 -0.216497 -0.216286 -0.975618 -0.412765 -0.916127  \n",
       "3 -0.216497 -0.216286  1.024991 -0.412765  1.091552  \n",
       "4 -0.216497 -0.216286 -0.975618 -0.412765 -0.916127  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dfTrain.drop('AdoptionSpeed', axis = 1)\n",
    "y = dfTrain[['AdoptionSpeed']]\n",
    "\n",
    "#convert all feature columns to float\n",
    "Xcols = X.columns.tolist()\n",
    "for col in Xcols:\n",
    "    X[col]= X[col].astype(float)\n",
    "\n",
    "#split train data into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "#print shape of train/test variables\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "dfXtrain = pd.DataFrame(X_train)\n",
    "dfXtest = pd.DataFrame(X_test)\n",
    "\n",
    "dfXtrain.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
